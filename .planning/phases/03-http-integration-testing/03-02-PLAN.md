---
phase: 03-http-integration-testing
plan: 02
type: execute
wave: 2
depends_on: [03-01]
files_modified:
  - tests/test_http_utilities.py
  - tests/test_http_errors.py
  - tests/test_http_concurrency.py
autonomous: true
requirements: [TEST-03, TEST-04, TEST-05]

must_haves:
  truths:
    - "list_form_fields returns correct results when called over HTTP for Word, Excel, and PDF fixtures"
    - "Malformed JSON body returns 400 with JSON-RPC parse error"
    - "Missing jsonrpc field returns 400 with validation error"
    - "Invalid tool name returns error in the JSON-RPC result"
    - "Three concurrent tool calls all succeed and return correct results without cross-contamination"
  artifacts:
    - path: "tests/test_http_utilities.py"
      provides: "HTTP tests for list_form_fields utility across Word, Excel, PDF"
      min_lines: 40
    - path: "tests/test_http_errors.py"
      provides: "Deeper HTTP error scenario tests beyond Phase 2 protocol tests"
      min_lines: 40
    - path: "tests/test_http_concurrency.py"
      provides: "Concurrent request tests proving stateless design"
      min_lines: 50
  key_links:
    - from: "tests/test_http_utilities.py"
      to: "tests/conftest.py"
      via: "mcp_session fixture, call_tool, parse_tool_result"
      pattern: "mcp_session|call_tool|parse_tool_result"
    - from: "tests/test_http_concurrency.py"
      to: "tests/conftest.py"
      via: "mcp_session fixture with concurrent threading.Thread calls"
      pattern: "mcp_session|threading"
---

<objective>
Add HTTP tests for utility tools, deeper error scenarios, and concurrent requests — completing the Phase 3 test coverage.

Purpose: Prove that utility tools work over HTTP (TEST-03), the server returns proper errors for malformed requests (TEST-04), and the stateless design handles concurrent requests correctly (TEST-05).
Output: test_http_utilities.py, test_http_errors.py, test_http_concurrency.py — three focused test files.
</objective>

<execution_context>
@/home/sarturko/.claude/get-shit-done/workflows/execute-plan.md
@/home/sarturko/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-http-integration-testing/03-RESEARCH.md
@.planning/phases/03-http-integration-testing/03-01-SUMMARY.md
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write HTTP utility and error tests</name>
  <files>tests/test_http_utilities.py, tests/test_http_errors.py</files>
  <action>
**tests/test_http_utilities.py** — Tests for TEST-03 (utility tools over HTTP).

Include AGPL license header. Import `call_tool`, `parse_tool_result` from conftest.py (pytest auto-discovers them).

Tests:
1. `test_list_form_fields_word(mcp_session)` — call list_form_fields with `{"file_path": "tests/fixtures/table_questionnaire.docx"}` over HTTP. Assert 200. Parse result. Assert result is a dict with a "fields" key (or whatever the actual return structure is). Compare against direct call: `from src.tools_extract import list_form_fields; direct = list_form_fields(file_path=...)`. Assert `http_result == direct`.

2. `test_list_form_fields_excel(mcp_session)` — Same pattern with `tests/fixtures/vendor_assessment.xlsx`.

3. `test_list_form_fields_pdf(mcp_session)` — Same pattern with `tests/fixtures/simple_form.pdf`.

4. `test_list_form_fields_nonexistent_file(mcp_session)` — Call with a file path that doesn't exist. The response should contain an error (either HTTP error or tool error in the result content). Assert the response indicates failure gracefully.

Note: `extract_text` does not exist as an MCP tool yet (per research). Do NOT test it. The 4 tests above satisfy TEST-03 for existing utility tools.

**tests/test_http_errors.py** — Tests for TEST-04 (deeper HTTP error scenarios beyond Phase 2).

Include AGPL license header. Imports: `json`, `starlette.testclient.TestClient`, conftest helpers.

For tests requiring raw HTTP without a session (malformed body tests), use `_fresh_app()` from conftest and create a TestClient inline (similar to how test_invalid_origin_returns_403 works in test_http_protocol.py — since malformed requests happen before a session is established, the mcp_session fixture isn't appropriate).

Tests:
1. `test_malformed_json_returns_400()` — POST `/mcp` with `content=b"{broken"` and MCP_HEADERS. Assert 400. Assert response body has JSON-RPC error with code -32700 (Parse error). Use `_fresh_app()` + inline TestClient.

2. `test_missing_jsonrpc_field_returns_400()` — POST `/mcp` with `json={"method": "initialize", "id": 1}` (no "jsonrpc" field). Assert 400. Assert JSON-RPC error body. Use `_fresh_app()` + inline TestClient.

3. `test_invalid_tool_name_returns_error(mcp_session)` — Call a non-existent tool "nonexistent_tool" via call_tool. The response should be 200 (JSON-RPC succeeds) but the result should contain an error (tool not found). Parse the SSE response and check for error content.

4. `test_tool_with_missing_required_args(mcp_session)` — Call "extract_structure_compact" with empty arguments `{}` (missing required file_path). Assert the response contains an error about missing arguments.

Import `_fresh_app` from conftest (or import mcp and _json_rpc_404_handler directly to build it inline — whichever keeps lines lower). Also import `MCP_HEADERS` and `INIT_BODY` from conftest.

Keep each file under 200 lines. Prefer concise test bodies.
  </action>
  <verify>
Run `pytest tests/test_http_utilities.py tests/test_http_errors.py -v` — all tests pass. Run `wc -l tests/test_http_utilities.py tests/test_http_errors.py` — both under 200 lines.
  </verify>
  <done>
test_http_utilities.py has 4 passing tests confirming list_form_fields works over HTTP for all 3 file types plus error case. test_http_errors.py has 4 passing tests for malformed JSON, missing fields, invalid tool, and bad arguments. Both files under 200 lines.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write concurrent request tests and run full regression</name>
  <files>tests/test_http_concurrency.py</files>
  <action>
**tests/test_http_concurrency.py** — Tests for TEST-05 (concurrent requests).

Include AGPL license header. Imports: `threading`, `json`, conftest helpers (`call_tool`, `parse_tool_result`).

Tests:
1. `test_concurrent_list_form_fields(mcp_session)` — Spawn 3 threads, each calling `list_form_fields` with a different file type:
   - Thread 1: `tests/fixtures/table_questionnaire.docx` (key: "word")
   - Thread 2: `tests/fixtures/vendor_assessment.xlsx` (key: "excel")
   - Thread 3: `tests/fixtures/simple_form.pdf` (key: "pdf")

   Each thread stores `(status_code, parsed_result)` in a shared `results` dict and any exceptions in an `errors` dict. Use unique `request_id` per thread (1, 2, 3).

   After join(timeout=30): assert no errors, assert all 3 results present, assert all status codes are 200. Then verify each result matches a direct call for that file (import `list_form_fields` from `src.tools_extract`).

2. `test_concurrent_extract_structure(mcp_session)` — Spawn 2 threads calling `extract_structure_compact` with different files:
   - Thread 1: `tests/fixtures/table_questionnaire.docx`
   - Thread 2: `tests/fixtures/vendor_assessment.xlsx`

   Assert both succeed with 200, results match direct calls, no cross-contamination (word result doesn't contain excel data and vice versa).

3. `test_concurrent_same_file(mcp_session)` — Spawn 3 threads all calling `extract_structure_compact` with the same file (`table_questionnaire.docx`). Assert all 3 return identical results — proving no interference when concurrent requests hit the same underlying data.

Keep under 200 lines. Use a shared `_run_concurrent(mcp_session, calls)` helper function within the file if needed to reduce duplication. Each `calls` entry: `(tool_name, args, key)`.

After writing the file, run the full test suite:
- `pytest tests/ -x -q` to confirm all tests pass (217 existing + ~17 new from Plans 01 and 02)
- If any test fails, fix it.
  </action>
  <verify>
Run `pytest tests/test_http_concurrency.py -v` — all 3 tests pass. Run `pytest tests/ -x -q` — full suite passes (should be ~234 tests). Run `wc -l tests/test_http_concurrency.py` — under 200 lines.
  </verify>
  <done>
test_http_concurrency.py has 3 passing concurrent request tests. Full test suite passes with zero regressions. All new test files under 200 lines. Phase 3 requirements TEST-03, TEST-04, TEST-05 satisfied.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/ -x -q` — complete test suite passes (217 existing + ~17 new)
2. `wc -l tests/test_http_utilities.py tests/test_http_errors.py tests/test_http_concurrency.py` — all under 200 lines
3. list_form_fields works over HTTP for Word, Excel, PDF (TEST-03)
4. Malformed JSON, missing fields, invalid tools all return proper errors (TEST-04)
5. Concurrent requests succeed without cross-contamination (TEST-05)
</verification>

<success_criteria>
- test_http_utilities.py: 4 passing tests for utility tools over HTTP
- test_http_errors.py: 4 passing tests for deeper error scenarios
- test_http_concurrency.py: 3 passing tests for concurrent requests
- Full test suite passes with zero regressions
- All files under 200 lines
</success_criteria>

<output>
After completion, create `.planning/phases/03-http-integration-testing/03-02-SUMMARY.md`
</output>
